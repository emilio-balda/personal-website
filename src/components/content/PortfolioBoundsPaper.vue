<template>
  <div>
    <!-- <img src="../../assets/portfolio/bounds-paper/title.png" alt="drawing" height="100"> -->

    <h1>Project Goal</h1>
    <p>
      Publish a theoretical paper about the generalization capabilities of deep neural networks under the presence of a malicious attacker modifying the inputs of the network.
    </p>

    <h1>Solution</h1>

    <p>
      I used tools from statistical learning theory to derive theoretical bounds that scaled better with the input dimension and the number of classes with respect to other existing bounds.
      The paper was published in AISTATS 2020.
      See the full paper [here](https://proceedings.mlr.press/v108/balda20a).
    </p>

    <h1>Tech Stack</h1>
    <!-- <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Python-logo-notext.svg/182px-Python-logo-notext.svg.png" alt="drawing" height="100">
    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/ab/TensorFlow_logo.svg/330px-TensorFlow_logo.svg.png" alt="drawing" height="100"> -->

    <p>
      I used TensorFlow to train neural networks and visualize the theoretical results in the following simulation (extracted from the paper).
    </p>

    <!-- <img src="../../assets/portfolio/bounds-paper/simulation.png"> -->
  </div>
</template>
